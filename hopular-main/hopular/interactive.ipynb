{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\disk\\\\AI\\\\2022_AI_Chanllenage\\\\maneuver-classification\\\\hopular-main\\\\hopular'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopfield_layers_master.hflayers import Hopfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract base class of a dataset to be used in Hopular.\n",
    "    \"\"\"\n",
    "\n",
    "    class CheckpointMode(Enum):\n",
    "        \"\"\"\n",
    "        Enumeration of available checkpoint modes used during the training and validation of Hopular.\n",
    "        \"\"\"\n",
    "        MIN = r'min'\n",
    "        MAX = r'max'\n",
    "\n",
    "    def encode_sample(self,\n",
    "                      sample: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode all features of a single sample depending on their respective feature type.\n",
    "\n",
    "        :param sample: sample to be encoded\n",
    "        :return: encoded sample\n",
    "        \"\"\"\n",
    "        sample = sample.view(-1)\n",
    "        assert len(sample) == len(self.feature_numeric) + len(self.feature_discrete), r'Invalid sample to encode!'\n",
    "\n",
    "        # Encode sample features according to feature type.\n",
    "        sample_encoded = []\n",
    "        for index, (datum, size) in enumerate(zip(sample, self.sizes)):\n",
    "            if index in self.feature_numeric:\n",
    "                sample_encoded.append(datum.view(-1))\n",
    "            else:\n",
    "                sample_encoded.append(torch.zeros(size).view(-1))\n",
    "                sample_encoded[-1][datum.int().item()] = 1\n",
    "        return torch.cat(sample_encoded, dim=0)\n",
    "\n",
    "    @property\n",
    "    def feature_mean(self) -> Optional[torch.Tensor]:\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def feature_stdv(self) -> Optional[torch.Tensor]:\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def sizes(self) -> Tuple[int, ...]:\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def split_train(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def split_validation(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def split_test(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def feature_numeric(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def feature_discrete(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def target_numeric(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def target_discrete(self) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def checkpoint_mode(self) -> CheckpointMode:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    \"\"\"\n",
    "    Data module encapsulating a dataset to be used in Hopular, providing data loading and masking capabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset: BaseDataset,\n",
    "                 batch_size: Optional[int] = None,\n",
    "                 super_sample_factor: int = 1,\n",
    "                 noise_probability: float = 0.15,\n",
    "                 mask_probability: float = 0.80,\n",
    "                 replace_probability: float = 0.10,\n",
    "                 num_workers: int = 0):\n",
    "        \"\"\"\n",
    "        Initialize a data module from a dataset.\n",
    "\n",
    "        :param dataset: dataset to be encapsulated by the data module\n",
    "        :param batch_size: sample count of a single mini-batch\n",
    "        :param super_sample_factor: multiplicity of the training set\n",
    "        :param noise_probability: probability of selecting an input feature for the self-supervised loss\n",
    "        :param mask_probability: probability of masking out a selected input feature\n",
    "        :param replace_probability: probability of replacing a selected input feature with a randomly drawn feature\n",
    "        :param num_workers: worker count of the data loaders\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.__batch_size = None if batch_size < 1 else batch_size\n",
    "        self.__super_sample_factor = super_sample_factor\n",
    "        self.__noise_probability = noise_probability\n",
    "        self.__mask_probability = mask_probability\n",
    "        self.__replace_probability = replace_probability\n",
    "        self.__num_workers = num_workers\n",
    "        assert 0 <= self.__noise_probability <= 1.0, r'Invalid noise probability!'\n",
    "        assert 0 <= self.__mask_probability <= 1.0, r'Invalid mask probability!'\n",
    "        assert 0 <= self.__replace_probability <= 1.0, r'Invalid replacement probability!'\n",
    "        assert (self.__mask_probability + self.__replace_probability) <= 1.0, r'Invalid mask/replacement probabilities!'\n",
    "\n",
    "        self.dims = self.dataset.shape[1:]\n",
    "        self.memory = None\n",
    "        self._has_setup_memory = False\n",
    "        self.__data_train = None\n",
    "        self.__data_validation = None\n",
    "        self.__data_test = None\n",
    "\n",
    "        # Register hyperparameters for logging.\n",
    "        self.save_hyperparameters(ignore=[r'dataset'])\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_and_noise_collate(\n",
    "            samples: List[Tuple[torch.Tensor, ...]],\n",
    "            mean: Optional[torch.Tensor],\n",
    "            stdv: Optional[torch.Tensor],\n",
    "            sizes: torch.Tensor,\n",
    "            noise_probability: float,\n",
    "            mask_probability: float,\n",
    "            replace_probability: float,\n",
    "            target_discrete: torch.Tensor,\n",
    "            target_numeric: torch.Tensor,\n",
    "            feature_discrete: torch.Tensor,\n",
    "            exclude_targets: bool) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Pre-process samples to be used in Hopular training and inference.\n",
    "\n",
    "        :param samples: collection of samples to be pre-processed\n",
    "        :param mean: feature means used for feature shifting\n",
    "        :param stdv: feature standard deviations used for feature scaling\n",
    "        :param sizes: feature sizes (class count for discrete features)\n",
    "        :param noise_probability: probability of selecting an input feature for the self-supervised loss\n",
    "        :param mask_probability: probability of masking out a selected input feature\n",
    "        :param replace_probability: probability of replacing a selected input feature with a randomly drawn feature\n",
    "        :param target_discrete: indices of discrete targets\n",
    "        :param target_numeric: indices of continuous targets\n",
    "        :param feature_discrete: indices of discrete features\n",
    "        :param exclude_targets: completely mask out targets\n",
    "        :return: masked samples, masking positions, unmasked samples, missing positions and original sample indices\n",
    "        \"\"\"\n",
    "        samples_collated = {}\n",
    "        for sample in samples:\n",
    "            for sample_index, sample_element in enumerate(sample):\n",
    "                samples_collated.setdefault(sample_index, []).append(sample_element)\n",
    "        samples_collated = tuple(torch.stack(\n",
    "            samples_collated[sample_index], dim=0\n",
    "        ) for sample_index in sorted(samples_collated))\n",
    "        feature_boundaries = torch.cumsum(torch.as_tensor([0] + sizes), dim=0)\n",
    "        feature_boundaries = zip(feature_boundaries[:-1], feature_boundaries[1:])\n",
    "\n",
    "        # Compute noise mask.\n",
    "        noise_mask = torch.ones(samples_collated[0].shape[0], len(sizes))\n",
    "        if noise_probability > 0:\n",
    "            noise_mask = torch.dropout(noise_mask, p=1.0 - noise_probability, train=True)\n",
    "            noise_mask = noise_mask != 0\n",
    "        else:\n",
    "            noise_mask = noise_mask == 0\n",
    "\n",
    "        # Scale sample features according to feature statistics and add optional noise.\n",
    "        samples_modified = []\n",
    "        for index, (start, end) in enumerate(feature_boundaries):\n",
    "\n",
    "            # Standardize attributes.\n",
    "            if index not in feature_discrete:\n",
    "                if mean is not None:\n",
    "                    assert not np.isnan(mean[index])\n",
    "                    samples_collated[0][:, start:end] = samples_collated[0][:, start:end] - mean[index]\n",
    "                if stdv is not None and stdv[index] > 0:\n",
    "                    assert not np.isnan(stdv[index])\n",
    "                    samples_collated[0][:, start:end] = samples_collated[0][:, start:end] / stdv[index]\n",
    "\n",
    "            # Encode features and targets accordingly and introduce optional noise.\n",
    "            if exclude_targets and (index in target_discrete or index in target_numeric):\n",
    "                current_sample = torch.cat((\n",
    "                    torch.zeros(len(samples_collated[0]), end - start),\n",
    "                    torch.ones(len(samples_collated[0]), 1)\n",
    "                ), dim=1)\n",
    "                samples_modified.append(current_sample)\n",
    "            else:\n",
    "                samples_modified.append(torch.cat((\n",
    "                    samples_collated[0][:, start:end],\n",
    "                    torch.zeros(len(samples_collated[0]), 1)\n",
    "                ), dim=1))\n",
    "\n",
    "                if noise_mask[:, index].any():\n",
    "                    current_mask = noise_mask[:, index]\n",
    "                    noise_feature = torch.rand(current_mask.sum())\n",
    "                    noise_zero = noise_feature < mask_probability\n",
    "                    noise_replace = mask_probability <= noise_feature\n",
    "                    noise_replace.logical_and_(noise_feature < (mask_probability + replace_probability))\n",
    "                    if noise_zero.any():\n",
    "                        current_feature = samples_modified[-1][current_mask]\n",
    "                        current_feature[noise_zero, :-1] = 0.0\n",
    "                        current_feature[noise_zero, -1] = 1.0\n",
    "                        samples_modified[-1][current_mask] = current_feature\n",
    "                    if noise_replace.any():\n",
    "                        current_feature = samples_modified[-1][current_mask]\n",
    "                        if index in feature_discrete:\n",
    "                            current_feature[noise_replace, :-1] = torch.nn.functional.one_hot(\n",
    "                                input=torch.randint(low=0, high=end - start, size=(noise_replace.sum(),)),\n",
    "                                num_classes=sizes[index]\n",
    "                            ).to(dtype=samples_modified[-1].dtype)\n",
    "                            samples_modified[-1][current_mask] = current_feature\n",
    "                        else:\n",
    "                            current_feature[noise_replace, :-1] = torch.randn(\n",
    "                                noise_replace.sum(), end - start)\n",
    "                            samples_modified[-1][current_mask] = current_feature\n",
    "\n",
    "            # Mask out missing features.\n",
    "            missing_mask = torch.as_tensor([]) if len(samples_collated) <= 1 else samples_collated[1][:, index]\n",
    "            missing_mask_count = missing_mask.sum()\n",
    "            if missing_mask_count > 0:\n",
    "                missing_sample = torch.zeros(missing_mask_count, end - start + 1)\n",
    "                samples_modified[-1][missing_mask] = missing_sample\n",
    "\n",
    "        # Adapt noise mask to include targets.\n",
    "        if len(target_discrete) > 0:\n",
    "            noise_mask[:, target_discrete] = True\n",
    "        if len(target_numeric) > 0:\n",
    "            noise_mask[:, target_numeric] = True\n",
    "\n",
    "        return torch.cat(samples_modified, dim=1), noise_mask, *samples_collated\n",
    "\n",
    "    def _get_subset(self,\n",
    "                    indices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get the specified subset from the dataset.\n",
    "\n",
    "        :param indices: indices of the subset samples\n",
    "        :return: specified subset\n",
    "        \"\"\"\n",
    "        data, data_missing = [], []\n",
    "        for index in indices:\n",
    "            current_sample = self.dataset[index]\n",
    "            data.append(current_sample[0])\n",
    "            data_missing.append(current_sample[1])\n",
    "        return torch.stack(data, dim=0), torch.stack(data_missing, dim=0)\n",
    "\n",
    "    def setup(self,\n",
    "              stage: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set up the specified stage of the data module.\n",
    "\n",
    "        :param stage: stage to set up\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if stage in (TrainerFn.FITTING, r'memory'):\n",
    "            assert self.dataset.split_train is not None, r'No training samples specified!'\n",
    "            data_train = self._get_subset(indices=self.dataset.split_train)\n",
    "            self.__data_train = ConcatDataset([\n",
    "                TensorDataset(*data_train, self.dataset.split_train) for _ in range(self.__super_sample_factor)\n",
    "            ])\n",
    "            if self.memory is None:\n",
    "                self.memory = self.scale_and_noise_collate(\n",
    "                    samples=list(zip(*data_train)),\n",
    "                    mean=self.dataset.feature_mean,\n",
    "                    stdv=self.dataset.feature_stdv,\n",
    "                    sizes=self.dataset.sizes,\n",
    "                    noise_probability=0.0,\n",
    "                    mask_probability=0.0,\n",
    "                    replace_probability=0.0,\n",
    "                    target_discrete=self.dataset.target_discrete,\n",
    "                    target_numeric=self.dataset.target_numeric,\n",
    "                    feature_discrete=self.dataset.feature_discrete,\n",
    "                    exclude_targets=False\n",
    "                )[0]\n",
    "        if stage in (TrainerFn.FITTING, TrainerFn.VALIDATING):\n",
    "            assert self.dataset.split_validation is not None, r'No validation samples specified!'\n",
    "            data_validation = self._get_subset(indices=self.dataset.split_validation)\n",
    "            self.__data_validation = TensorDataset(*data_validation)\n",
    "        elif stage == TrainerFn.TESTING:\n",
    "            assert self.dataset.split_test is not None, r'No test samples specified!'\n",
    "            data_test = self._get_subset(indices=self.dataset.split_test)\n",
    "            self.__data_test = TensorDataset(*data_test)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Prepare and get the data loader for the training subset.\n",
    "\n",
    "        :return: data loader for the training subset\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.__data_train,\n",
    "            batch_size=len(self.__data_train) if self.__batch_size is None else self.__batch_size,\n",
    "            pin_memory=self.trainer.gpus is not None,\n",
    "            num_workers=self.__num_workers,\n",
    "            persistent_workers=self.__num_workers > 0,\n",
    "            collate_fn=partial(\n",
    "                self.scale_and_noise_collate,\n",
    "                mean=self.dataset.feature_mean,\n",
    "                stdv=self.dataset.feature_stdv,\n",
    "                sizes=self.dataset.sizes,\n",
    "                noise_probability=self.__noise_probability,\n",
    "                mask_probability=self.__mask_probability,\n",
    "                replace_probability=self.__replace_probability,\n",
    "                target_discrete=self.dataset.target_discrete,\n",
    "                target_numeric=self.dataset.target_numeric,\n",
    "                feature_discrete=self.dataset.feature_discrete,\n",
    "                exclude_targets=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Prepare and get the data loader for the validation subset.\n",
    "\n",
    "        :return: data loader for the validation subset\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.__data_validation,\n",
    "            batch_size=len(self.__data_validation),\n",
    "            pin_memory=self.trainer.gpus is not None,\n",
    "            num_workers=self.__num_workers,\n",
    "            persistent_workers=self.__num_workers > 0,\n",
    "            collate_fn=partial(\n",
    "                self.scale_and_noise_collate,\n",
    "                mean=self.dataset.feature_mean,\n",
    "                stdv=self.dataset.feature_stdv,\n",
    "                sizes=self.dataset.sizes,\n",
    "                noise_probability=0.0,\n",
    "                mask_probability=0.0,\n",
    "                replace_probability=0.0,\n",
    "                target_discrete=self.dataset.target_discrete,\n",
    "                target_numeric=self.dataset.target_numeric,\n",
    "                feature_discrete=self.dataset.feature_discrete,\n",
    "                exclude_targets=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Prepare and get the data loader for the test subset.\n",
    "\n",
    "        :return: data loader for the test subset\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.__data_test,\n",
    "            batch_size=len(self.__data_test),\n",
    "            pin_memory=self.trainer.gpus is not None,\n",
    "            num_workers=self.__num_workers,\n",
    "            persistent_workers=self.__num_workers > 0,\n",
    "            collate_fn=partial(\n",
    "                self.scale_and_noise_collate,\n",
    "                mean=self.dataset.feature_mean,\n",
    "                stdv=self.dataset.feature_stdv,\n",
    "                sizes=self.dataset.sizes,\n",
    "                noise_probability=0.0,\n",
    "                mask_probability=0.0,\n",
    "                replace_probability=0.0,\n",
    "                target_discrete=self.dataset.target_discrete,\n",
    "                target_numeric=self.dataset.target_numeric,\n",
    "                feature_discrete=self.dataset.feature_discrete,\n",
    "                exclude_targets=True\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_data_module(cls,\n",
    "                        data_module: DataModule,\n",
    "                        **kwargs: Dict[str, Any]) -> r'Hopular':\n",
    "    \"\"\"\n",
    "    Initialize Hopular from a pre-instantiated data module.\n",
    "\n",
    "    :param data_module: module encapsulating a dataset\n",
    "    :param kwargs: additional keyword arguments used for initializing Hopular\n",
    "    :return: new Hopular instance\n",
    "    \"\"\"\n",
    "    data_module.setup(stage=r'memory')\n",
    "    return cls(\n",
    "        input_sizes=data_module.dataset.sizes,\n",
    "        target_discrete=data_module.dataset.target_discrete.tolist(),\n",
    "        target_numeric=data_module.dataset.target_numeric.tolist(),\n",
    "        feature_discrete=data_module.dataset.feature_discrete.tolist(),\n",
    "        memory=data_module.memory,\n",
    "        memory_ids=data_module.dataset.split_train,\n",
    "        feature_mean=data_module.dataset.feature_mean,\n",
    "        feature_stdv=data_module.dataset.feature_stdv,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\pytorch_lighting\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from auxiliary.data import find_dataset, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.mode = r'optim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = find_dataset(name='GlassIdentificationDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "super_sample_factor = 0.5\n",
    "noise_probability=0.5\n",
    "mask_probability=0.5\n",
    "replace_probability=0.5\n",
    "num_workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index, num_splits = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "        dataset=dataset(split_index=split_index),\n",
    "        batch_size=batch_size,\n",
    "        super_sample_factor=super_sample_factor,\n",
    "        noise_probability=noise_probability,\n",
    "        mask_probability=mask_probability,\n",
    "        replace_probability=replace_probability,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size=32\n",
    "hidden_size=0\n",
    "hidden_size_factor=1.0\n",
    "num_heads=8\n",
    "scaling_factor=1.0\n",
    "input_dropout=0.1\n",
    "lookup_dropout=0.1\n",
    "output_dropout=0.01\n",
    "memory_ratio=1.0\n",
    "num_blocks=8\n",
    "\n",
    "# Optimizer-specific arguments.\n",
    "initial_feature_loss_weight=1.0\n",
    "final_feature_loss_weight=0.0\n",
    "learning_rate=1e-3\n",
    "gamma=1.0\n",
    "betas=(0.9, 0.999)\n",
    "weight_decay=0.1\n",
    "lookup_steps=1\n",
    "lookup_ratio=0.005\n",
    "warmup_ratio=0.0\n",
    "num_steps_per_cycle=10000\n",
    "cold_restart=True\n",
    "synchronous_weights = False\n",
    "asynchronous_weights=not synchronous_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_data_module() missing 1 required positional argument: 'cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\disk\\AI\\2022_AI_Chanllenage\\maneuver-classification\\hopular-main\\hopular\\interactive.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=0'>1</a>\u001b[0m hopular_model \u001b[39m=\u001b[39m from_data_module(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=2'>3</a>\u001b[0m                 \u001b[39m# Hopular specific arguments.\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=3'>4</a>\u001b[0m                 data_module\u001b[39m=\u001b[39;49mdata_module,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=4'>5</a>\u001b[0m                 feature_size\u001b[39m=\u001b[39;49mfeature_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=5'>6</a>\u001b[0m                 hidden_size\u001b[39m=\u001b[39;49mhidden_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=6'>7</a>\u001b[0m                 hidden_size_factor\u001b[39m=\u001b[39;49mhidden_size_factor,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=7'>8</a>\u001b[0m                 num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=8'>9</a>\u001b[0m                 scaling_factor\u001b[39m=\u001b[39;49mscaling_factor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=9'>10</a>\u001b[0m                 input_dropout\u001b[39m=\u001b[39;49minput_dropout,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=10'>11</a>\u001b[0m                 lookup_dropout\u001b[39m=\u001b[39;49mlookup_dropout,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=11'>12</a>\u001b[0m                 output_dropout\u001b[39m=\u001b[39;49moutput_dropout,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=12'>13</a>\u001b[0m                 memory_ratio\u001b[39m=\u001b[39;49mmemory_ratio,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=13'>14</a>\u001b[0m                 num_blocks\u001b[39m=\u001b[39;49mnum_blocks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=15'>16</a>\u001b[0m                 \u001b[39m# Optimizer-specific arguments.\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=16'>17</a>\u001b[0m                 initial_feature_loss_weight\u001b[39m=\u001b[39;49minitial_feature_loss_weight,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=17'>18</a>\u001b[0m                 final_feature_loss_weight\u001b[39m=\u001b[39;49mfinal_feature_loss_weight,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=18'>19</a>\u001b[0m                 learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=19'>20</a>\u001b[0m                 gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=20'>21</a>\u001b[0m                 betas\u001b[39m=\u001b[39;49mbetas,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=21'>22</a>\u001b[0m                 weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=22'>23</a>\u001b[0m                 lookup_steps\u001b[39m=\u001b[39;49mlookup_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=23'>24</a>\u001b[0m                 lookup_ratio\u001b[39m=\u001b[39;49mlookup_ratio,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=24'>25</a>\u001b[0m                 warmup_ratio\u001b[39m=\u001b[39;49mwarmup_ratio,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=25'>26</a>\u001b[0m                 num_steps_per_cycle\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=26'>27</a>\u001b[0m                 cold_restart\u001b[39m=\u001b[39;49mcold_restart,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=27'>28</a>\u001b[0m                 asynchronous_weights\u001b[39m=\u001b[39;49masynchronous_weights,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=28'>29</a>\u001b[0m                 synchronous_weights\u001b[39m=\u001b[39;49msynchronous_weights\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=29'>30</a>\u001b[0m             )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/disk/AI/2022_AI_Chanllenage/maneuver-classification/hopular-main/hopular/interactive.ipynb#ch0000022?line=30'>31</a>\u001b[0m hopular_model\u001b[39m.\u001b[39mreset_parameters()\n",
      "\u001b[1;31mTypeError\u001b[0m: from_data_module() missing 1 required positional argument: 'cls'"
     ]
    }
   ],
   "source": [
    "hopular_model = from_data_module(\n",
    "\n",
    "                # Hopular specific arguments.\n",
    "                data_module=data_module,\n",
    "                feature_size=feature_size,\n",
    "                hidden_size=hidden_size,\n",
    "                hidden_size_factor=hidden_size_factor,\n",
    "                num_heads=num_heads,\n",
    "                scaling_factor=scaling_factor,\n",
    "                input_dropout=input_dropout,\n",
    "                lookup_dropout=lookup_dropout,\n",
    "                output_dropout=output_dropout,\n",
    "                memory_ratio=memory_ratio,\n",
    "                num_blocks=num_blocks,\n",
    "\n",
    "                # Optimizer-specific arguments.\n",
    "                initial_feature_loss_weight=initial_feature_loss_weight,\n",
    "                final_feature_loss_weight=final_feature_loss_weight,\n",
    "                learning_rate=learning_rate,\n",
    "                gamma=gamma,\n",
    "                betas=betas,\n",
    "                weight_decay=weight_decay,\n",
    "                lookup_steps=lookup_steps,\n",
    "                lookup_ratio=lookup_ratio,\n",
    "                warmup_ratio=warmup_ratio,\n",
    "                num_steps_per_cycle=10000,\n",
    "                cold_restart=cold_restart,\n",
    "                asynchronous_weights=asynchronous_weights,\n",
    "                synchronous_weights=synchronous_weights\n",
    "            )\n",
    "hopular_model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07353bba56e5e710f8fc84a09af5331174a3a69e5ff26553d09ef6596433b1ca"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
